# ML |ç”¨ Python ä¸­çš„ SMOTE å’Œ Miss ç®—æ³•å¤„ç†ä¸å¹³è¡¡æ•°æ®

> åŸæ–‡:[https://www . geeksforgeeks . org/ml-å¤„ç†ä¸å¹³è¡¡æ•°æ®-é‡‡ç”¨ python ä¸­çš„ smote-miss ç®—æ³•/](https://www.geeksforgeeks.org/ml-handling-imbalanced-data-with-smote-and-near-miss-algorithm-in-python/)

åœ¨æœºå™¨å­¦ä¹ å’Œæ•°æ®ç§‘å­¦ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸é‡åˆ°ä¸€ä¸ªå«åš**ä¸å¹³è¡¡æ•°æ®åˆ†å¸ƒ**çš„æœ¯è¯­ï¼Œé€šå¸¸å‘ç”Ÿåœ¨ä¸€ä¸ªç±»ä¸­çš„è§‚å¯Ÿå€¼æ¯”å…¶ä»–ç±»é«˜å¾—å¤šæˆ–ä½å¾—å¤šçš„æ—¶å€™ã€‚ç”±äºæœºå™¨å­¦ä¹ ç®—æ³•å€¾å‘äºé€šè¿‡å‡å°‘è¯¯å·®æ¥æé«˜ç²¾åº¦ï¼Œå› æ­¤å®ƒä»¬ä¸è€ƒè™‘ç±»åˆ†å¸ƒã€‚è¿™ä¸ªé—®é¢˜åœ¨**æ¬ºè¯ˆæ£€æµ‹**ã€**å¼‚å¸¸æ£€æµ‹**ã€**é¢éƒ¨è¯†åˆ«**ç­‰ä¾‹å­ä¸­æ™®éå­˜åœ¨ã€‚

æ ‡å‡†çš„æœ€å¤§ä¼¼ç„¶æŠ€æœ¯ï¼Œå¦‚å†³ç­–æ ‘å’Œé€»è¾‘å›å½’ï¼Œåå‘äº**å¤šæ•°**ç±»ï¼Œå®ƒä»¬å€¾å‘äºå¿½ç•¥å°‘æ•°ç±»ã€‚ä»–ä»¬å€¾å‘äºåªé¢„æµ‹å¤šæ•°é˜¶çº§ï¼Œå› æ­¤ï¼Œä¸å¤šæ•°é˜¶çº§ç›¸æ¯”ï¼Œå°‘æ•°é˜¶çº§æœ‰å¾ˆå¤§çš„é”™è¯¯åˆ†ç±»ã€‚ç”¨æ›´ä¸“ä¸šçš„è¯æ¥è¯´ï¼Œå¦‚æœæˆ‘ä»¬çš„æ•°æ®é›†ä¸­å­˜åœ¨ä¸å¹³è¡¡çš„æ•°æ®åˆ†å¸ƒï¼Œé‚£ä¹ˆæˆ‘ä»¬çš„æ¨¡å‹å°±æ›´å®¹æ˜“å‡ºç°å°‘æ•°æ°‘æ—å¯ä»¥å¿½ç•¥æˆ–å¾ˆå°‘**å›å¿†**çš„æƒ…å†µã€‚

**ä¸å¹³è¡¡æ•°æ®å¤„ç†æŠ€æœ¯:**ä¸»è¦æœ‰ 2 ç§ä¸»è¦ç®—æ³•è¢«å¹¿æ³›ç”¨äºå¤„ç†ä¸å¹³è¡¡çš„ç±»åˆ†å¸ƒã€‚

1.  é‡å‡»
2.  æœªé‚äº‹ä»¶ç®—æ³•

## **SMOTE(åˆæˆå°‘æ•°è¿‡é‡‡æ ·æŠ€æœ¯)â€“è¿‡é‡‡æ ·**

åˆæˆå°‘æ•°è¿‡é‡‡æ ·æŠ€æœ¯æ˜¯è§£å†³ä¸å¹³è¡¡é—®é¢˜æœ€å¸¸ç”¨çš„è¿‡é‡‡æ ·æ–¹æ³•ä¹‹ä¸€ã€‚
æ—¨åœ¨é€šè¿‡å¤åˆ¶éšæœºå¢åŠ çš„å°‘æ•°æ°‘æ—é˜¶çº§ä¾‹å­æ¥å¹³è¡¡é˜¶çº§åˆ†å¸ƒã€‚
SMOTE åœ¨ç°æœ‰çš„å°‘æ•°æ°‘æ—å®ä¾‹ä¹‹é—´åˆæˆæ–°çš„å°‘æ•°æ°‘æ—å®ä¾‹ã€‚å®ƒé€šè¿‡çº¿æ€§æ’å€¼ä¸ºå°‘æ•°æ°‘æ—ç­çº§ç”Ÿæˆ**è™šæ‹Ÿè®­ç»ƒè®°å½•ã€‚è¿™äº›åˆæˆè®­ç»ƒè®°å½•æ˜¯é€šè¿‡ä¸ºå°‘æ•°ç±»ä¸­çš„æ¯ä¸ªç¤ºä¾‹éšæœºé€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ª k è¿‘é‚»æ¥ç”Ÿæˆçš„ã€‚åœ¨è¿‡é‡‡æ ·è¿‡ç¨‹ä¹‹åï¼Œæ•°æ®è¢«é‡å»ºï¼Œå¹¶ä¸”å‡ ä¸ªåˆ†ç±»æ¨¡å‹å¯ä»¥è¢«åº”ç”¨äºè¢«å¤„ç†çš„æ•°æ®ã€‚
**æ›´å¤šå…³äº SMOTE ç®—æ³•å·¥ä½œåŸç†çš„æ·±åˆ»è§è§£ï¼****

*   **ç¬¬ä¸€æ­¥:**è®¾ç½®å°‘æ•°ç±»é›†åˆ **A** ï¼Œå¯¹äºæ¯ä¸ª **![$x \in A$](img/2f06eb7ecdc745cf7c3b466549cd005e.png "Rendered by QuickLaTeX.com")** ï¼Œé€šè¿‡è®¡ç®— **x** ä¸é›†åˆ **A** ä¸­æ¯éš”ä¸€ä¸ªæ ·æœ¬ä¹‹é—´çš„**æ¬§æ°è·ç¦»**å¾—åˆ° x çš„ **k è¿‘é‚»ã€‚***   **ç¬¬äºŒæ­¥:**é‡‡æ ·ç‡ **N** æ ¹æ®ä¸å¹³è¡¡æ¯”ä¾‹è®¾ç½®ã€‚å¯¹äºæ¯ä¸ª **![$x \in A$](img/2f06eb7ecdc745cf7c3b466549cd005e.png "Rendered by QuickLaTeX.com")** ï¼Œ **N** ä¸ªä¾‹å­(å³ x1ï¼Œx2ï¼Œâ€¦xn)ä»å…¶ k ä¸ªæœ€è¿‘é‚»ä¸­éšæœºé€‰æ‹©ï¼Œå®ƒä»¬æ„æˆé›†åˆ![$A_1$](img/29230380d734d15abc3f514f47524324.png "Rendered by QuickLaTeX.com")ã€‚*   **Step 3:** For each example **![$x_k \in A_1$](img/67f6f5e2ead01f9bff8425012aa88a1b.png "Rendered by QuickLaTeX.com")** (k=1, 2, 3â€¦N), the following formula is used to generate a new example:
    ![$x' = x + rand(0, 1) * \mid x - x_k \mid$](img/1c94f75415c9e774d74dd6051b445964.png "Rendered by QuickLaTeX.com")
    in which rand(0, 1) represents the random number between 0 and 1.

    ## **æ¥è¿‘ç¼ºå¤±ç®—æ³•â€“æ¬ é‡‡æ ·**

    æ¥è¿‘ç¼ºå¤±æ˜¯ä¸€ç§æ¬ é‡‡æ ·æŠ€æœ¯ã€‚å®ƒæ—¨åœ¨é€šè¿‡éšæœºæ¶ˆé™¤å¤šæ•°ç±»ç¤ºä¾‹æ¥å¹³è¡¡ç±»åˆ†å¸ƒã€‚å½“ä¸¤ä¸ªä¸åŒç±»çš„å®ä¾‹å½¼æ­¤éå¸¸æ¥è¿‘æ—¶ï¼Œæˆ‘ä»¬ç§»é™¤å¤šæ•°ç±»çš„å®ä¾‹æ¥å¢åŠ ä¸¤ä¸ªç±»ä¹‹é—´çš„ç©ºé—´ã€‚è¿™æœ‰åŠ©äºåˆ†ç±»è¿‡ç¨‹ã€‚
    ä¸ºäº†é˜²æ­¢**ä¿¡æ¯ä¸¢å¤±çš„é—®é¢˜**åœ¨å¤§å¤šæ•°æ¬ é‡‡æ ·æŠ€æœ¯ä¸­ï¼Œ**è¿‘é‚»**æ–¹æ³•è¢«å¹¿æ³›ä½¿ç”¨ã€‚
    **å…³äºè¿‘é‚»æ–¹æ³•å·¥ä½œçš„åŸºæœ¬ç›´è§‰å¦‚ä¸‹:**

    *   **æ­¥éª¤ 1:** è¯¥æ–¹æ³•é¦–å…ˆæ‰¾åˆ°å¤šæ•°ç±»çš„æ‰€æœ‰å®ä¾‹å’Œå°‘æ•°ç±»çš„å®ä¾‹ä¹‹é—´çš„è·ç¦»ã€‚è¿™é‡Œï¼Œå¤šæ•°ç±»å°†è¢«æ¬ é‡‡æ ·ã€‚*   **æ­¥éª¤ 2:** ç„¶åï¼Œé€‰æ‹©ä¸å°‘æ•°æ°‘æ—ç±»ä¸­çš„å®ä¾‹è·ç¦»æœ€å°çš„å¤šæ•°æ°‘æ—ç±»çš„ **n** ä¸ªå®ä¾‹ã€‚*   **Step 3:** If there are k instances in the minority class, the nearest method will result in **k*n** instances of the majority class.

    **ä¸ºäº†åœ¨å¤šæ•°ç±»ä¸­æ‰¾åˆ° n ä¸ªæœ€æ¥è¿‘çš„å®ä¾‹ï¼Œæœ‰å‡ ç§åº”ç”¨æ¥è¿‘ç¼ºå¤±ç®—æ³•çš„å˜ä½“:**

    1.  **æ¥è¿‘ç¼ºå¤±â€“ç‰ˆæœ¬ 1 :** å®ƒé€‰æ‹©å¤šæ•°ç±»çš„æ ·æœ¬ï¼Œå¯¹äºè¿™äº›æ ·æœ¬ï¼Œåˆ° k **æœ€è¿‘çš„å°‘æ•°ç±»**å®ä¾‹çš„å¹³å‡è·ç¦»æœ€å°ã€‚
    2.  **æ¥è¿‘ç¼ºå¤±â€“ç‰ˆæœ¬ 2 :** å®ƒé€‰æ‹©å¤šæ•°ç±»çš„æ ·æœ¬ï¼Œå¯¹äºè¿™äº›æ ·æœ¬ï¼Œåˆ° k **æœ€è¿œçš„**å°‘æ•°ç±»å®ä¾‹çš„å¹³å‡è·ç¦»æœ€å°ã€‚
    3.  **æ¥è¿‘ç¼ºå¤±â€“ç‰ˆæœ¬ 3 :** å®ƒåˆ†ä¸¤æ­¥å·¥ä½œã€‚é¦–å…ˆï¼Œå¯¹äºæ¯ä¸ªå°‘æ•°æ°‘æ—ç±»å®ä¾‹ï¼Œå®ƒä»¬çš„ **M æœ€è¿‘é‚»**å°†è¢«å­˜å‚¨ã€‚æœ€åï¼Œé€‰æ‹©åˆ° N ä¸ªæœ€è¿‘é‚»çš„å¹³å‡è·ç¦»æœ€å¤§çš„å¤šæ•°ç±»å®ä¾‹ã€‚

    **æœ¬æ–‡æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£å’Œå®è·µå¦‚ä½•åœ¨ä¸åŒçš„ä¸å¹³è¡¡æ•°æ®å¤„ç†æŠ€æœ¯ä¹‹é—´åšå‡ºæœ€ä½³é€‰æ‹©ã€‚**

    ### åŠ è½½åº“å’Œæ•°æ®æ–‡ä»¶

    æ•°æ®é›†ç”±ä¿¡ç”¨å¡äº¤æ˜“ç»„æˆã€‚è¯¥æ•°æ®é›†åœ¨ 284ï¼Œ807 ç¬”äº¤æ˜“ä¸­æœ‰ **492 ç¬”æ¬ºè¯ˆäº¤æ˜“ã€‚è¿™ä½¿å¾—å®ƒé«˜åº¦ä¸å¹³è¡¡ï¼Œæ­£ç±»(æ¬ºè¯ˆ)å æ‰€æœ‰äº¤æ˜“çš„ 0.172%ã€‚
    æ•°æ®é›†å¯ä»¥ä» **[è¿™é‡Œ](https://www.kaggle.com/mlg-ulb/creditcardfraud)** ä¸‹è½½ã€‚**

    ```
    # import necessary modulesÂ 
    import pandasÂ  as pd
    import matplotlib.pyplot as plt
    import numpy as np
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import confusion_matrix, classification_report

    # load the data set
    data = pd.read_csv('creditcard.csv')

    # print info about columns in the dataframe
    print(data.info())
    ```

    **è¾“å‡º:**

    ```
    RangeIndex: 284807 entries, 0 to 284806
    Data columns (total 31 columns):
    Time      284807 non-null float64
    V1        284807 non-null float64
    V2        284807 non-null float64
    V3        284807 non-null float64
    V4        284807 non-null float64
    V5        284807 non-null float64
    V6        284807 non-null float64
    V7        284807 non-null float64
    V8        284807 non-null float64
    V9        284807 non-null float64
    V10       284807 non-null float64
    V11       284807 non-null float64
    V12       284807 non-null float64
    V13       284807 non-null float64
    V14       284807 non-null float64
    V15       284807 non-null float64
    V16       284807 non-null float64
    V17       284807 non-null float64
    V18       284807 non-null float64
    V19       284807 non-null float64
    V20       284807 non-null float64
    V21       284807 non-null float64
    V22       284807 non-null float64
    V23       284807 non-null float64
    V24       284807 non-null float64
    V25       284807 non-null float64
    V26       284807 non-null float64
    V27       284807 non-null float64
    V28       284807 non-null float64
    Amount    284807 non-null float64
    Class     284807 non-null int64

    ```

    ```
    # normalise the amount column
    data['normAmount'] = StandardScaler().fit_transform(np.array(data['Amount']).reshape(-1, 1))

    # drop Time and Amount columns as they are not relevant for prediction purposeÂ 
    data = data.drop(['Time', 'Amount'], axis = 1)

    # as you can see there are 492 fraud transactions.
    data['Class'].value_counts()
    ```

    **è¾“å‡º:**

    ```
           0    284315
           1       492

    ```

    ### å°†æ•°æ®åˆ†æˆæµ‹è¯•é›†å’Œè®­ç»ƒé›†

    ```
    from sklearn.model_selection import train_test_split

    # split into 70:30 ration
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)

    # describes info about train and test set
    print("Number transactions X_train dataset: ", X_train.shape)
    print("Number transactions y_train dataset: ", y_train.shape)
    print("Number transactions X_test dataset: ", X_test.shape)
    print("Number transactions y_test dataset: ", y_test.shape)
    ```

    **è¾“å‡º:**

    ```
          Number transactions X_train dataset:  (199364, 29)
          Number transactions y_train dataset:  (199364, 1)
          Number transactions X_test dataset:  (85443, 29)
          Number transactions y_test dataset:  (85443, 1)

    ```

    ### ç°åœ¨åœ¨ä¸å¤„ç†ä¸å¹³è¡¡çš„ç±»åˆ†å¸ƒçš„æƒ…å†µä¸‹è®­ç»ƒæ¨¡å‹

    ```
    # logistic regression object
    lr = LogisticRegression()

    # train the model on train set
    lr.fit(X_train, y_train.ravel())

    predictions = lr.predict(X_test)

    # print classification report
    print(classification_report(y_test, predictions))
    ```

    **è¾“å‡º:**

    ```

                    precision   recall   f1-score  support

               0       1.00      1.00      1.00     85296
               1       0.88      0.62      0.73       147

        accuracy                           1.00     85443
       macro avg       0.94      0.81      0.86     85443
    weighted avg       1.00      1.00      1.00     85443

    ```

    **å‡†ç¡®ç‡å‡ºæ¥æ˜¯ 100%ï¼Œä½†æ˜¯ä½ æ³¨æ„åˆ°ä»€ä¹ˆå¥‡æ€ªçš„å—ï¼Ÿ**
    å¯¹å°‘æ•°æ°‘æ—é˜¶çº§çš„å›å¿†åœ¨å¾ˆå°‘ã€‚è¯æ˜è¯¥æ¨¡å‹æ›´åå‘å¤šæ•°äººé˜¶å±‚ã€‚æ‰€ä»¥ï¼Œè¯æ˜è¿™ä¸æ˜¯æœ€å¥½çš„æ¨¡å¼ã€‚
    ç°åœ¨ï¼Œæˆ‘ä»¬å°†åº”ç”¨ä¸åŒçš„**ä¸å¹³è¡¡æ•°æ®å¤„ç†æŠ€æœ¯**ï¼Œçœ‹çœ‹å®ƒä»¬çš„å‡†ç¡®æ€§å’Œå¬å›ç»“æœã€‚

    ### ä½¿ç”¨ SMOTE ç®—æ³•

    æ‚¨å¯ä»¥ä»[è¿™é‡Œ](https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html)æŸ¥çœ‹æ‰€æœ‰å‚æ•°ã€‚

    ```
    print("Before OverSampling, counts of label '1': {}".format(sum(y_train == 1)))
    print("Before OverSampling, counts of label '0': {} \n".format(sum(y_train == 0)))

    # import SMOTE module from imblearn library
    # pip install imblearn (if you don't have imblearn in your system)
    from imblearn.over_sampling import SMOTE
    sm = SMOTE(random_state = 2)
    X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())

    print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))
    print('After OverSampling, the shape of train_y: {} \n'.format(y_train_res.shape))

    print("After OverSampling, counts of label '1': {}".format(sum(y_train_res == 1)))
    print("After OverSampling, counts of label '0': {}".format(sum(y_train_res == 0)))
    ```

    **è¾“å‡º:**

    ```
    Before OverSampling, counts of label '1': [345]
    Before OverSampling, counts of label '0': [199019] 

    After OverSampling, the shape of train_X: (398038, 29)
    After OverSampling, the shape of train_y: (398038, ) 

    After OverSampling, counts of label '1': 199019
    After OverSampling, counts of label '0': 199019

    ```

    **çœ‹ï¼ã€SMOTE ç®—æ³•å¯¹å°‘æ•°å®ä¾‹è¿›è¡Œè¿‡é‡‡æ ·ï¼Œä½¿å…¶ç­‰äºå¤šæ•°ç±»ã€‚ä¸¤ä¸ªç±»åˆ«çš„è®°å½•æ•°é‡ç›¸ç­‰ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå°‘æ•°æ°‘æ—å·²ç»å¢åŠ åˆ°å¤šæ•°æ°‘æ—çš„æ€»æ•°ã€‚
    ç°åœ¨çœ‹çœ‹åº”ç”¨ SMOTE ç®—æ³•(è¿‡é‡‡æ ·)åçš„å‡†ç¡®ç‡å’Œå¬å›ç‡ç»“æœã€‚**

    #### é¢„æµ‹å’Œå›å¿†

    ```
    lr1 = LogisticRegression()
    lr1.fit(X_train_res, y_train_res.ravel())
    predictions = lr1.predict(X_test)

    # print classification report
    print(classification_report(y_test, predictions))
    ```

    **è¾“å‡º:**

    ```
                    precision   recall   f1-score  support

               0       1.00      0.98      0.99     85296
               1       0.06      0.92      0.11       147

        accuracy                           0.98     85443
       macro avg       0.53      0.95      0.55     85443
    weighted avg       1.00      0.98      0.99     85443

    ```

    **å“‡**æˆ‘ä»¬ç›¸æ¯”ä¹‹å‰çš„æ¨¡å‹å‡†ç¡®ç‡é™ä½åˆ°äº† 98%ï¼Œä½†æ˜¯å°ä¼—ç±»çš„å¬å›å€¼ä¹Ÿæå‡åˆ°äº† 92 %ã€‚ä¸å‰ä¸€ä¸ªç›¸æ¯”ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æ¨¡å‹ã€‚å›å¿†å¾ˆæ£’ã€‚
    ç°åœ¨ï¼Œæˆ‘ä»¬å°†åº”ç”¨ NearMiss æŠ€æœ¯å¯¹å¤šæ•°ç±»è¿›è¡Œæ¬ é‡‡æ ·ï¼Œå¹¶æŸ¥çœ‹å…¶å‡†ç¡®æ€§å’Œå¬å›ç»“æœã€‚

    ### æ¥è¿‘ç¼ºå¤±ç®—æ³•:

    æ‚¨å¯ä»¥ä»[è¿™é‡Œ](https://imbalanced-learn.org/en/stable/generated/imblearn.under_sampling.NearMiss.html)æŸ¥çœ‹æ‰€æœ‰å‚æ•°ã€‚

    ```
    print("Before Undersampling, counts of label '1': {}".format(sum(y_train == 1)))
    print("Before Undersampling, counts of label '0': {} \n".format(sum(y_train == 0)))

    # apply near miss
    from imblearn.under_sampling import NearMiss
    nr = NearMiss()

    X_train_miss, y_train_miss = nr.fit_sample(X_train, y_train.ravel())

    print('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape))
    print('After Undersampling, the shape of train_y: {} \n'.format(y_train_miss.shape))

    print("After Undersampling, counts of label '1': {}".format(sum(y_train_miss == 1)))
    print("After Undersampling, counts of label '0': {}".format(sum(y_train_miss == 0)))
    ```

    **è¾“å‡º:**

    ```
    Before Undersampling, counts of label '1': [345]
    Before Undersampling, counts of label '0': [199019] 

    After Undersampling, the shape of train_X: (690, 29)
    After Undersampling, the shape of train_y: (690, ) 

    After Undersampling, counts of label '1': 345
    After Undersampling, counts of label '0': 345

    ```

    **æ¥è¿‘ç¼ºå¤±ç®—æ³•**å¯¹å¤šæ•°å®ä¾‹è¿›è¡Œäº†æ¬ é‡‡æ ·ï¼Œä½¿å…¶ç­‰äºå¤šæ•°ç±»ã€‚åœ¨è¿™é‡Œï¼Œå¤šæ•°ç±»å·²ç»å‡å°‘åˆ°å°‘æ•°ç±»çš„æ€»æ•°ï¼Œè¿™æ ·ä¸¤ä¸ªç±»çš„è®°å½•æ•°å°†ç›¸ç­‰ã€‚

    #### é¢„æµ‹å’Œå›å¿†

    ```
    # train the model on train set
    lr2 = LogisticRegression()
    lr2.fit(X_train_miss, y_train_miss.ravel())
    predictions = lr2.predict(X_test)

    # print classification report
    print(classification_report(y_test, predictions))
    ```

    **è¾“å‡º:**

    ```
                   precision    recall   f1-score   support

               0       1.00      0.56      0.72     85296
               1       0.00      0.95      0.01       147

        accuracy                           0.56     85443
       macro avg       0.50      0.75      0.36     85443
    weighted avg       1.00      0.56      0.72     85443

    ```

    è¯¥æ¨¡å‹æ¯”ç¬¬ä¸€ä¸ªæ¨¡å‹æ›´å¥½ï¼Œå› ä¸ºå®ƒåˆ†ç±»æ›´å¥½ï¼Œè€Œä¸”å°‘æ•°ç±»çš„å¬å›ç‡ä¸º 95 %ã€‚ä½†ç”±äºå¤šæ•°ç±»çš„æ¬ é‡‡æ ·ï¼Œå…¶å¬å›ç‡å·²é™è‡³ 56 %ã€‚å› æ­¤ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒSMOTE ç»™äº†æˆ‘å¾ˆé«˜çš„å‡†ç¡®æ€§å’Œå¬å›ç‡ï¼Œæˆ‘å°†ç»§ç»­ä½¿ç”¨è¯¥æ¨¡å‹ï¼ğŸ™‚